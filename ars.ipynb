{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7796657,"sourceType":"datasetVersion","datasetId":4564591},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U langchain_experimental langchain_openai\n!pip install -q -U sentence-transformers\n!pip install -q -U pinecone-text\n!pip install -q -U langchain-pinecone\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install -q -U accelerate\n!pip install -q -U langchain_community","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-21T22:46:09.595663Z","iopub.execute_input":"2024-04-21T22:46:09.595997Z","iopub.status.idle":"2024-04-21T22:47:54.071301Z","shell.execute_reply.started":"2024-04-21T22:46:09.595970Z","shell.execute_reply":"2024-04-21T22:47:54.070009Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nbotocore 1.34.51 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import getpass\nimport os\nimport logging\n\nimport pandas as pd\nimport torch\nimport transformers\nimport bitsandbytes\nimport accelerate\n\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.document_loaders import DataFrameLoader\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_community.document_loaders.csv_loader import CSVLoader","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:47:54.073881Z","iopub.execute_input":"2024-04-21T22:47:54.074303Z","iopub.status.idle":"2024-04-21T22:48:01.500472Z","shell.execute_reply.started":"2024-04-21T22:47:54.074252Z","shell.execute_reply":"2024-04-21T22:48:01.499372Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"_ARS__MAX_NEW_TOKENS = 512\n_ARS__TEMPERATURE = 0.1\n_ARS__TOP_P = 0.1\n_ARS__K = 3\n_ARS__BREAKPOINT_THRESHOLD_TYPE = \"percentile\"\n_ARS__BREAKPOINT_THRESHOLD_AMOUNT = 95\n\nclass ARSDocumentLoader():\n    r\"\"\"\n    A class for loading documents from various file formats.\n\n    Currently supported file extensions: csv.\n\n    Example:\n        loader = ARSDocumentLoader()\n        documents, errors = loader.get_documents_from_files(\"file1.csv\", \"file2.csv\")\n    \"\"\"\n        \n    @classmethod\n    def get_documents_from_files(cls, *paths: str):\n        r\"\"\"\n        Retrieve documents from multiple files.\n\n        This method takes file paths as input, checks the existence of the files,\n        and extracts documents from supported file formats.\n\n        Args:\n            *paths (str): Variable-length list of file paths from which to load documents.\n\n        Returns:\n            List, List: A tuple containing a list of documents loaded from the files\n                and a list of errors encountered during the loading process.\n\n        Example:\n            documents, errors = ARSDocumentLoader.get_documents_from_files(\"file1.csv\", \"file2.csv\")\n        \"\"\"\n        documents = []\n        errors = []\n\n        for path in paths:\n            if not os.path.exists(path):\n                logging.error(f\"File does not exist for path: {path}\")\n                errors.append({\"path\": path, \"error\": f\"File does not exist for path: {path}\"})\n                continue\n\n            file_extension = path.split('.')[-1].lower()\n\n            match file_extension:\n                case \"csv\":\n                    docs = cls.__get_documents_from_csv(path)\n                case _:\n                    logging.error(f\"File extension is not supported: {file_extension}\")\n                    errors.append({\"path\": path, \"error\": f\"File extension is not supported: {extension}\"})\n                    continue\n\n            documents.extend(docs)\n\n        return documents, errors\n                   \n    @classmethod\n    def __get_documents_from_csv(cls, path: str):\n        r\"\"\"\n        Wrapper for langchain CSVLoader\n        https://python.langchain.com/docs/integrations/document_loaders/csv/\n        \"\"\"\n        return CSVLoader(path).load()\n\nclass ARS():\n    r\"\"\"\n    The RAG (Retrieval-Augmented Generation) model combines retrieval-based and generation-based approaches\n    to generate responses to user queries or prompts.\n    \n    Args:\n        llm_model (str): Path to the llama-3-chat-hf language model.\n        embedding_model (str): Name of the embedding model on huggingface.\n        index_name (str): Name of the index on Pinecone.\n        max_new_tokens (int, optional): The maximum number of tokens to generate. Default is 512.\n        temperature (float, optional): The sampling temperature. Default is 0.1.\n        top_p (float, optional): The nucleus sampling top-p threshold. Default is 0.1.\n        k (int, optional): The number of documents to retrieve for context. Default is 3.\n        \n    Notes:\n        This class relies on several external dependencies. \n        For a list of dependencies and installation instructions, please refer to the project's README.md file.\n    \"\"\"\n    \n    def __init__(self, llama_model_path: str, embedding_model: str, index_name: str, \n                 max_new_tokens: int = __MAX_NEW_TOKENS, temperature: float = __TEMPERATURE, \n                 top_p: float = __TOP_P, k: int = __K, \n                 breakpoint_threshold_type: str = __BREAKPOINT_THRESHOLD_TYPE,\n                 breakpoint_threshold_amount: int = __BREAKPOINT_THRESHOLD_AMOUNT):\n        self.document_loader = ARSDocumentLoader\n        \n        self.pipeline = transformers.pipeline(\n            \"text-generation\",\n            model=llama_model_path,\n            model_kwargs={\n                \"torch_dtype\": torch.float16,\n                \"quantization_config\": {\"load_in_8bit\": True},\n                \"low_cpu_mem_usage\": True,\n            }\n        )\n        \n        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_model)\n        \n        self.text_splitter = SemanticChunker(\n            self.embedding_model,\n            breakpoint_threshold_type=breakpoint_threshold_type,\n            breakpoint_threshold_amount=breakpoint_threshold_amount\n        )\n        \n        self.index = PineconeVectorStore(embedding=self.embedding_model, \n                                         index_name=index_name)\n        \n        self.max_new_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.k = k\n        \n    def change_config(self, **kwargs):\n        \"\"\"\n        Change the configuration parameters of the Article Retrieval System (ARS).\n\n        Args:\n            **kwargs: Keyword arguments representing the configuration parameters to be changed.\n                Supported parameters:\n                - k (int, optional): The number of documents to retrieve for context.\n                - max_new_tokens (int, optional): The maximum number of tokens to generate.\n                - temperature (float, optional): The sampling temperature.\n                - top_p (float, optional): The nucleus sampling top-p threshold.\n                - breakpoint_threshold_type (str, optional): The type of breakpoint threshold for semantic chunking.\n                - breakpoint_threshold_amount (float, optional): The amount of breakpoint threshold for semantic chunking.\n\n        Returns:\n            None\n        \"\"\"\n        self.k = kwargs.get(\"k\", self.k)\n        self.max_new_tokens = kwargs.get(\"max_new_tokens\", self.max_new_tokens)\n        self.temperature = kwargs.get(\"temperature\", self.temperature)\n        self.top_p = kwargs.get(\"top_p\", self.top_p)\n        self.breakpoint_threshold_type = kwargs.get(\"breakpoint_threshold_type\", self.breakpoint_threshold_type)\n        self.breakpoint_threshold_amount = kwargs.get(\"breakpoint_threshold_amount\", self.breakpoint_threshold_amount)\n    \n    \n    def add_context_from_files(self, *paths: str):\n        r\"\"\"\n        Adds context from files to the vector database.\n        \n        Args:\n            *paths (str): Paths to the files to be processed.\n\n        Returns:\n            tuple: A tuple containing processed documents and errors.\n                - list: Documents obtained from files.\n                - list: Errors encountered during processing files.\n        \"\"\"\n        \n        documents, errors = self.document_loader.get_documents_from_files(*paths)\n        \n        splitted_documents = self.text_splitter.split_documents(documents)\n        \n        self.index.add_documents(splitted_documents)\n            \n        return documents, errors    \n    \n    def query(self, query: str, **kwargs):\n        r\"\"\"\n        Query the model for an answer to the given query.\n\n        Args:\n            query (str): The query to be answered.\n            **kwargs:\n                k (int, optional): The number of documents to retrieve for context.\n                max_new_tokens (int, optional): The maximum number of tokens to generate.\n                temperature (float, optional): The sampling temperature.\n                top_p (float, optional): The nucleus sampling top-p threshold.\n                \n        Returns:\n            str: The generated answer.\n        \"\"\"\n        k = kwargs.get(\"k\", self.k)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", self.max_new_tokens)\n        temperature = kwargs.get(\"temperature\", self.temperature)\n        top_p = kwargs.get(\"top_p\", self.top_p)\n        \n        if not self.pipeline:\n            logging.error(\"Language model pipeline is not initialized\")\n            raise RuntimeError(\"Language model pipeline is not initialized\")\n        \n        user_context = self.__get_context_from_index(query, k)\n        \n        prompt = self.__create_prompt(query, user_context)\n                               \n        terminators = [\n            self.pipeline.tokenizer.eos_token_id,\n            self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n        ]\n\n        outputs = self.pipeline(\n            prompt,\n            max_new_tokens=max_new_tokens,\n            eos_token_id=terminators,\n            do_sample=True,\n            temperature=temperature,\n            top_p=top_p,\n        )\n        \n        return outputs[0][\"generated_text\"][len(prompt):]\n    \n    \n    def __get_context_from_index(self, query: str, k: int):\n        r\"\"\"\n        Get user context from the search index based on the query.\n\n        Args:\n            query (str): The query for similarity search.\n            k (int): The number of documents to retrieve.\n\n        Returns:\n            str: The generated user context including retrieved documents and the query.\n        \"\"\"\n        \n        docs = self.index.similarity_search(query, k=k)\n        \n        user_context = \"\"\n\n        for doc in docs:\n            user_context += \"\\n\" + doc.page_content\n            \n        return user_context\n    \n    \n    def __create_prompt(self, query: str, context: str):\n        r\"\"\"\n        Creates prompt using query and context.\n\n        Args:\n            query (str): The question/query to be included in the prompt.\n            context (str): The context to be included in the prompt.\n\n        Returns:\n            str: The generated prompt for model input.\n        \"\"\"\n        \n        user_message = f\"Context: {context} \\n Question:{query}\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": \"Using provided context answer the question without saying that it is based on the context!\"},\n            {\"role\": \"user\", \"content\": user_message},\n        ]\n\n        return self.pipeline.tokenizer.apply_chat_template(\n                messages, \n                tokenize=False, \n                add_generation_prompt=True\n        )","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:48:01.502580Z","iopub.execute_input":"2024-04-21T22:48:01.503457Z","iopub.status.idle":"2024-04-21T22:48:01.535137Z","shell.execute_reply.started":"2024-04-21T22:48:01.503415Z","shell.execute_reply":"2024-04-21T22:48:01.534071Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:48:01.537692Z","iopub.execute_input":"2024-04-21T22:48:01.538432Z","iopub.status.idle":"2024-04-21T22:48:19.933779Z","shell.execute_reply.started":"2024-04-21T22:48:01.538371Z","shell.execute_reply":"2024-04-21T22:48:19.932989Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdin","text":"Pinecone API Key: ····································\n"}]},{"cell_type":"code","source":"INPUT_PATH = \"/kaggle/input/1300-towards-datascience-medium-articles-dataset/\"\nINDEX_NAME = \"rag\"\nMODEL_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\nEMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:48:19.934891Z","iopub.execute_input":"2024-04-21T22:48:19.935200Z","iopub.status.idle":"2024-04-21T22:48:19.939607Z","shell.execute_reply.started":"2024-04-21T22:48:19.935156Z","shell.execute_reply":"2024-04-21T22:48:19.938724Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"rag = ARS(MODEL_PATH, EMBEDDING_MODEL, INDEX_NAME)\n_, _ = rag.add_context_from_files(INPUT_PATH + 'medium.csv')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-21T22:48:19.940738Z","iopub.execute_input":"2024-04-21T22:48:19.941013Z","iopub.status.idle":"2024-04-21T22:52:44.258931Z","shell.execute_reply.started":"2024-04-21T22:48:19.940980Z","shell.execute_reply":"2024-04-21T22:52:44.258093Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-21 22:48:21.576214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-21 22:48:21.576312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-21 22:48:21.706870: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b71a206aae9e4000a8118449c298628a"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20293ca986964706b6b09070fe483158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1adb6cfa08004b61b31d658e36abe3d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4623b43d204694ae97f2360e769814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"611398e6a2bf4ec2b9b5dc8269246075"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"803bc1951d89483781dd9c8f116b1b0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef0fbb1b54e40b68f42509149750170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24251599b3b04e37b735cd2c7df6aada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a028196de1431389df91c959eda533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b515f5d23b45e48e209e7c249a6dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b47b251a0dd24ccda3ef35bb43e8ede1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0799246c51a24e2fa081e2843109931a"}},"metadata":{}}]},{"cell_type":"code","source":"result = rag.query(\"Will computers take over the world?\")\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:52:54.431933Z","iopub.execute_input":"2024-04-21T22:52:54.432692Z","iopub.status.idle":"2024-04-21T22:53:17.419021Z","shell.execute_reply.started":"2024-04-21T22:52:54.432654Z","shell.execute_reply":"2024-04-21T22:53:17.418082Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The question of whether computers will take over the world is a topic of ongoing debate and speculation. While advancements in AI have made significant progress in recent years, experts are still unclear how to develop actual intelligence. Even if deep learning matures to a point where neural networks are equivalent to the human brain, AI experts are still unsure how to develop actual intelligence.\n","output_type":"stream"}]},{"cell_type":"code","source":"result = rag.query(\"What is SQL?\")\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:53:17.420358Z","iopub.execute_input":"2024-04-21T22:53:17.420725Z","iopub.status.idle":"2024-04-21T22:53:26.395956Z","shell.execute_reply.started":"2024-04-21T22:53:17.420690Z","shell.execute_reply":"2024-04-21T22:53:26.395027Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"SQL is a standardised programming language designed for data storage and management. It allows one to create, parse, and manipulate data fast and easy.\n","output_type":"stream"}]},{"cell_type":"code","source":"result = rag.query(\"What is bootstrap?\")\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:53:26.399068Z","iopub.execute_input":"2024-04-21T22:53:26.399514Z","iopub.status.idle":"2024-04-21T22:53:47.536617Z","shell.execute_reply.started":"2024-04-21T22:53:26.399484Z","shell.execute_reply":"2024-04-21T22:53:47.535638Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Bootstrap is a powerful, computer-based method for statistical inference without relying on too many assumptions. It's a resampling method that involves independently sampling with replacement from an existing sample data with the same sample size, and performing inference among these resampled data.\n","output_type":"stream"}]}]}